{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9120f617",
   "metadata": {},
   "source": [
    "# define the module and gpt class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_k: int, dim_v: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_k)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_v)\n",
    "        #self.embedding=nn.Embedding() ##\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Tensor) -> Tensor:  ## 传入mask \n",
    "        query= self.q(query)\n",
    "        key= self.k(key)\n",
    "        value=self.v(value)\n",
    "\n",
    "        temp = query.bmm(key.transpose(1, 2))\n",
    "        scale = query.size(-1) ** 0.5\n",
    "\n",
    "        score=temp/scale\n",
    "        score=score+mask\n",
    "\n",
    "        softmax = f.softmax(score, dim=-1)\n",
    "        return softmax.bmm(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_k: int, dim_v: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_k, dim_v) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_v, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask) -> Tensor: ## 传入mask\n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value,mask) for h in self.heads], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(\n",
    "    seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tensor:\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / 1e4 ** (dim // dim_model)\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_input, dim_feedforward),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dim_feedforward, dim_input),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c999324",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1): ##不是在这里传入mask\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tensors0: Tensor, tensors1: Tensor, tensors2: Tensor, mask: Tensor) -> Tensor:\n",
    "        # Assume that the \"value\" tensor is given last, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        # self.mask=mask\n",
    "        return self.norm(tensors0 + self.dropout(self.sublayer(tensors0, tensors1, tensors2,mask))) ## 传入mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8473db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feed fowward network,Residual需要传入mask，这里不用，所以要分别开\n",
    "class Residual_ffn(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1): ##不传入mask\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tensors: Tensor) -> Tensor:\n",
    "        # Assume that the \"value\" tensor is given last, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        #self.mask=mask ##\n",
    "        return self.norm(tensors + self.dropout(self.sublayer(tensors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 6, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_k = dim_v = dim_model // num_heads\n",
    "        self.attention = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v), ## 传入mask\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout, \n",
    "        )\n",
    "        self.feed_forward = Residual_ffn(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor,mask: Tensor) -> Tensor: ##传入mask\n",
    "        src = self.attention(src, src, src,mask) ###传入mask\n",
    "        #return src \n",
    "        return self.feed_forward(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb862e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 8, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Tensor) -> Tensor: ##可以传入mask\n",
    "        seq_len, dimension = src.size(1), src.size(2)\n",
    "        pos=position_encoding(seq_len,dimension) #\n",
    "        pos=pos.cuda(1) # load data to gpu\n",
    "        src += pos\n",
    "        for layer in self.layers:\n",
    "            src = layer(src,mask)  ##传入mask\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a trial of gpt model testing\n",
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_encoder_layers: int = 4,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 8, \n",
    "        dim_feedforward: int = 2048//2, \n",
    "        dropout: float = 0.1, \n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "        n_vocab: int=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(n_vocab, dim_model)\n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_encoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(dim_model, n_vocab)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Tensor) -> Tensor: ##传入mask\n",
    "        emb=self.embedding(src)\n",
    "        enc=self.encoder(emb,mask) ##传入mask\n",
    "        out=self.out(enc)\n",
    "        \n",
    "        return out ##no need softmax, nn.cross_entropy take care of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11785966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7b865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02903460",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a802506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is demo data which is padded, can be used for training\n",
    "import numpy as np\n",
    "data=np.genfromtxt('sentence_100.csv').tolist()\n",
    "new_data= data*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919aebd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9caecf34",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b36ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a square matrix where the each row allows one word more to be seen\n",
    "\n",
    "def generate_masks(src):\n",
    "    seq_len= src.size(1)\n",
    "\n",
    "    pad_int= [int(seq_len-i) for i in src.count_nonzero(dim=1)]\n",
    "\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len) == 1) # Lower triangular matrix\n",
    "    mask = mask.float()\n",
    "    mask = mask.masked_fill(mask == 0, -1e9) # Convert zeros to -1e9\n",
    "    mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "\n",
    "    mask_arr=[]\n",
    "    for i in pad_int:\n",
    "      mask[:,-(i):]= -1e9\n",
    "      mask_arr.append(mask)\n",
    "\n",
    "    masks=torch.cat(tuple(mask_arr),dim=0)\n",
    "    masks=masks.reshape(src.size(0),seq_len,seq_len)\n",
    "    \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32bb1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8577f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d45f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change code to single gpu\n",
    "gpu=1\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = GPT(n_vocab=27656)\n",
    "\n",
    "#model = nn.DataParallel(model)\n",
    "#torch.cuda.set_device(gpu)\n",
    "model.cuda(1)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss().cuda(1)  ## this is for classification\n",
    "opt = torch.optim.SGD(model.parameters(), 1e-4)\n",
    "\n",
    "######\n",
    "def loss_masked(output, src):\n",
    "    nonpad_int=src.count_nonzero(dim=1)\n",
    "    # discard pad elements\n",
    "    res=[]\n",
    "    for k,item in enumerate(nonpad_int):\n",
    "        res.append(src[k][:int(item)])\n",
    "\n",
    "    loss_res=0\n",
    "    for i in range(src.size(0)):\n",
    "        loss_res+=loss_fn(output[i], src[i])\n",
    "\n",
    "    return loss_res/src.size(0)\n",
    "######\n",
    "\n",
    "batch_size = 128\n",
    "n=len(new_data)// batch_size\n",
    "\n",
    "# prepare for next word prediction\n",
    "t0=time.time()\n",
    "\n",
    "for i in range(n):\n",
    "    src=new_data[batch_size*i:batch_size*(i+1)]\n",
    "    src=torch.tensor(src).long()\n",
    "\n",
    "\n",
    "    _seq=src[:,:-1]\n",
    "    seq_=src[:,1:]\n",
    "    masks=generate_masks(_seq)\n",
    "\n",
    "    \n",
    "    #put to gpu\n",
    "    _seq=_seq.cuda(1)\n",
    "    seq_=seq_.cuda(1)\n",
    "    masks=masks.cuda(1)\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(_seq,masks)\n",
    "    loss=loss_masked(outputs,seq_) # next word prediction\n",
    "\n",
    "\n",
    "    # the part of padding loss should be removed before backprop\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "\n",
    "    print(loss.detach().item())\n",
    "    print(time.time()-t0)\n",
    "    \n",
    "    if i%10000==0: np.savetxt('./torch_save_model/gpt_loss_%d.csv'%(i), np.array([loss.detach().item()]))\n",
    "\n",
    "# save model parameters    \n",
    "#torch.save(model, \"./torch_save_model/model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90873858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
