{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "KxDU6uXEi7T-",
      "metadata": {
        "id": "KxDU6uXEi7T-"
      },
      "source": [
        "# preprocess wikipedia corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n89MoYXkkiwm",
      "metadata": {
        "id": "n89MoYXkkiwm"
      },
      "source": [
        "## load word piece model for tokeniztion usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j_A2hVwYi5K4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-12-22T11:39:22.511478Z",
          "iopub.status.busy": "2022-12-22T11:39:22.510808Z",
          "iopub.status.idle": "2022-12-22T11:39:28.264033Z",
          "shell.execute_reply": "2022-12-22T11:39:28.263252Z",
          "shell.execute_reply.started": "2022-12-22T11:39:22.511445Z"
        },
        "id": "j_A2hVwYi5K4",
        "outputId": "412308aa-fdd3-47cc-c962-7d45ea4f83e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://dtr-vip.ccg13.slc.paypalinc.com/artifactory/api/pypi/python/simple\n",
            "Requirement already satisfied: transformers in /projects/tanhuang/apps/tanhuang/research/.local/lib/python3.7/site-packages (4.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (5.1.0)\n",
            "Requirement already satisfied: sacremoses in /projects/tanhuang/apps/tanhuang/research/.local/lib/python3.7/site-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TWI2lIEdi_4L",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:39:28.272880Z",
          "iopub.status.busy": "2022-12-22T11:39:28.272431Z",
          "iopub.status.idle": "2022-12-22T11:39:29.904762Z",
          "shell.execute_reply": "2022-12-22T11:39:29.904140Z",
          "shell.execute_reply.started": "2022-12-22T11:39:28.272853Z"
        },
        "id": "TWI2lIEdi_4L",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QtCcgG6QkrzH",
      "metadata": {
        "id": "QtCcgG6QkrzH"
      },
      "source": [
        "## load corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JQazVT9kkfu7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:38:40.509789Z",
          "iopub.status.busy": "2022-12-22T11:38:40.508748Z",
          "iopub.status.idle": "2022-12-22T11:38:40.521437Z",
          "shell.execute_reply": "2022-12-22T11:38:40.520744Z",
          "shell.execute_reply.started": "2022-12-22T11:38:40.509752Z"
        },
        "id": "JQazVT9kkfu7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class utils(object):\n",
        "  def __init__(self,path=None):\n",
        "    self.path=path\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "  def preprocess_wiki(self, path):\n",
        "    f = open(self.path)\n",
        "    res = f.read().splitlines()\n",
        "    f.close()\n",
        "\n",
        "    vocab=set()\n",
        "    length=[]\n",
        "    sentence_arr=[]\n",
        "    for i in range(len(res)):\n",
        "      text=res[i]\n",
        "      tokenized_text = self.tokenizer.tokenize(text)\n",
        "      # remove sentence which is too short, too long\n",
        "      if 5<= len(tokenized_text)<=100:\n",
        "        sentence_arr.append(tokenized_text)\n",
        "        length.append(len(tokenized_text))\n",
        "        vocab.update(tokenized_text)\n",
        "\n",
        "    v2i={v: i for i, v in enumerate(sorted(vocab), start=1)}\n",
        "    v2i['<PAD>']=0\n",
        "    v2i[\"<SEP>\"] = len(v2i) # <GO> as start of sequence ,<SEP> as end of sequence\n",
        "    v2i[\"<GO>\"] = len(v2i) # the total number of tokens should include these special tokens: len(v2i)\n",
        "\n",
        "    i2v = {i: v for v, i in v2i.items()}  \n",
        "    return sentence_arr, v2i, i2v, max(length)\n",
        "\n",
        "  def token_to_idx(self,sentence_arr, v2i):\n",
        "    sentence_idx=[]\n",
        "    for i in range(len(sentence_arr)):\n",
        "      sentence_idx.append([v2i['<GO>']]+[v2i[item] for item in sentence_arr[i]]+[v2i['<SEP>']])\n",
        "    return sentence_idx\n",
        "\n",
        "  # add a pad_zero function to align the sentences of various length\n",
        "  def pad_zero(self, seqs, max_len):\n",
        "      PAD_ID = 0\n",
        "      padded = np.full((len(seqs), max_len), fill_value=PAD_ID, dtype=np.int32)\n",
        "      for i, seq in enumerate(seqs):\n",
        "          padded[i, :len(seq)] = seq\n",
        "      return padded\n",
        "\n",
        "  def get_idx_sentence(self):\n",
        "    sentence_arr, v2i, i2v, max_len= self.preprocess_wiki(self.path) #input is part of wiki data, for demo usage\n",
        "    sentence_idx = self.token_to_idx(sentence_arr, v2i)\n",
        "    # define idx for padding\n",
        "    PAD_ID= v2i['<PAD>']\n",
        "    # there is <GO> and <SEP> at start and ending of sentence, so the full length should be 100+2=102\n",
        "    sentence_idx_padded = self.pad_zero(sentence_idx,max_len+2)\n",
        "\n",
        "    return sentence_idx_padded.tolist(), v2i\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ziYud6Twn0tN",
      "metadata": {
        "id": "ziYud6Twn0tN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9120f617",
      "metadata": {
        "id": "9120f617"
      },
      "source": [
        "# define the module and gpt class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5c4953e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:38:41.510555Z",
          "iopub.status.busy": "2022-12-22T11:38:41.510302Z",
          "iopub.status.idle": "2022-12-22T11:38:42.060873Z",
          "shell.execute_reply": "2022-12-22T11:38:42.060257Z",
          "shell.execute_reply.started": "2022-12-22T11:38:41.510528Z"
        },
        "id": "c5c4953e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch.nn.functional as f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ca3d26",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:38:42.513291Z",
          "iopub.status.busy": "2022-12-22T11:38:42.513034Z",
          "iopub.status.idle": "2022-12-22T11:38:42.520652Z",
          "shell.execute_reply": "2022-12-22T11:38:42.520078Z",
          "shell.execute_reply.started": "2022-12-22T11:38:42.513266Z"
        },
        "id": "07ca3d26",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, dim_in: int, dim_k: int, dim_v: int):\n",
        "        super().__init__()\n",
        "        self.q = nn.Linear(dim_in, dim_k)\n",
        "        self.k = nn.Linear(dim_in, dim_k)\n",
        "        self.v = nn.Linear(dim_in, dim_v)\n",
        "        #self.embedding=nn.Embedding() ##\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Tensor) -> Tensor:  ## 传入mask \n",
        "        query= self.q(query)\n",
        "        key= self.k(key)\n",
        "        value=self.v(value)\n",
        "\n",
        "        temp = query.bmm(key.transpose(1, 2))\n",
        "        scale = query.size(-1) ** 0.5\n",
        "\n",
        "        score=temp/scale\n",
        "        score=score+mask\n",
        "\n",
        "        softmax = f.softmax(score, dim=-1)\n",
        "        return softmax.bmm(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ec0dc65",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:38:42.521945Z",
          "iopub.status.busy": "2022-12-22T11:38:42.521757Z",
          "iopub.status.idle": "2022-12-22T11:38:42.546754Z",
          "shell.execute_reply": "2022-12-22T11:38:42.546239Z",
          "shell.execute_reply.started": "2022-12-22T11:38:42.521923Z"
        },
        "id": "7ec0dc65",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads: int, dim_in: int, dim_k: int, dim_v: int):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [AttentionHead(dim_in, dim_k, dim_v) for _ in range(num_heads)]\n",
        "        )\n",
        "        self.linear = nn.Linear(num_heads * dim_v, dim_in)\n",
        "\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask) -> Tensor: ## 传入mask\n",
        "        return self.linear(\n",
        "            torch.cat([h(query, key, value,mask) for h in self.heads], dim=-1)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58cd2c04",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:38:42.547709Z",
          "iopub.status.busy": "2022-12-22T11:38:42.547520Z",
          "iopub.status.idle": "2022-12-22T11:38:42.572791Z",
          "shell.execute_reply": "2022-12-22T11:38:42.572267Z",
          "shell.execute_reply.started": "2022-12-22T11:38:42.547687Z"
        },
        "id": "58cd2c04",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def position_encoding(\n",
        "    seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\"),\n",
        ") -> Tensor:\n",
        "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
        "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
        "    phase = pos / 1e4 ** (dim // dim_model)\n",
        "\n",
        "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f70a440b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:38:42.574365Z",
          "iopub.status.busy": "2022-12-22T11:38:42.573849Z",
          "iopub.status.idle": "2022-12-22T11:38:42.599778Z",
          "shell.execute_reply": "2022-12-22T11:38:42.599249Z",
          "shell.execute_reply.started": "2022-12-22T11:38:42.574337Z"
        },
        "id": "f70a440b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(dim_input, dim_feedforward),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(dim_feedforward, dim_input),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c999324",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:38:42.625997Z",
          "iopub.status.busy": "2022-12-22T11:38:42.625795Z",
          "iopub.status.idle": "2022-12-22T11:38:42.631505Z",
          "shell.execute_reply": "2022-12-22T11:38:42.630849Z",
          "shell.execute_reply.started": "2022-12-22T11:38:42.625958Z"
        },
        "id": "1c999324",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1): ##不是在这里传入mask\n",
        "        super().__init__()\n",
        "        self.sublayer = sublayer\n",
        "        self.norm = nn.LayerNorm(dimension)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tensors0: Tensor, tensors1: Tensor, tensors2: Tensor, mask: Tensor) -> Tensor:\n",
        "        # Assume that the \"value\" tensor is given last, so we can compute the\n",
        "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
        "        # self.mask=mask\n",
        "        return self.norm(tensors0 + self.dropout(self.sublayer(tensors0, tensors1, tensors2,mask))) ## 传入mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8473db38",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:38:43.510316Z",
          "iopub.status.busy": "2022-12-22T11:38:43.509878Z",
          "iopub.status.idle": "2022-12-22T11:38:43.515698Z",
          "shell.execute_reply": "2022-12-22T11:38:43.515125Z",
          "shell.execute_reply.started": "2022-12-22T11:38:43.510287Z"
        },
        "id": "8473db38",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## feed fowward network,Residual需要传入mask，这里不用，所以要分别开\n",
        "class Residual_ffn(nn.Module):\n",
        "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1): ##不传入mask\n",
        "        super().__init__()\n",
        "        self.sublayer = sublayer\n",
        "        self.norm = nn.LayerNorm(dimension)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tensors: Tensor) -> Tensor:\n",
        "        # Assume that the \"value\" tensor is given last, so we can compute the\n",
        "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
        "        #self.mask=mask ##\n",
        "        return self.norm(tensors + self.dropout(self.sublayer(tensors)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b13c689b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:38:43.534055Z",
          "iopub.status.busy": "2022-12-22T11:38:43.533647Z",
          "iopub.status.idle": "2022-12-22T11:38:43.542318Z",
          "shell.execute_reply": "2022-12-22T11:38:43.541845Z",
          "shell.execute_reply.started": "2022-12-22T11:38:43.534028Z"
        },
        "id": "b13c689b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        dim_model: int = 512, \n",
        "        num_heads: int = 6, \n",
        "        dim_feedforward: int = 2048, \n",
        "        dropout: float = 0.1, \n",
        "    ):\n",
        "        super().__init__()\n",
        "        dim_k = dim_v = dim_model // num_heads\n",
        "        self.attention = Residual(\n",
        "            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v), ## 传入mask\n",
        "            dimension=dim_model,\n",
        "            dropout=dropout, \n",
        "        )\n",
        "        self.feed_forward = Residual_ffn(\n",
        "            feed_forward(dim_model, dim_feedforward),\n",
        "            dimension=dim_model,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "    def forward(self, src: Tensor,mask: Tensor) -> Tensor: ##传入mask\n",
        "        src = self.attention(src, src, src,mask) ###传入mask\n",
        "        #return src \n",
        "        return self.feed_forward(src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdb862e9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:38:43.543298Z",
          "iopub.status.busy": "2022-12-22T11:38:43.543119Z",
          "iopub.status.idle": "2022-12-22T11:38:43.569585Z",
          "shell.execute_reply": "2022-12-22T11:38:43.568876Z",
          "shell.execute_reply.started": "2022-12-22T11:38:43.543277Z"
        },
        "id": "cdb862e9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        num_layers: int = 6,\n",
        "        dim_model: int = 512, \n",
        "        num_heads: int = 8, \n",
        "        dim_feedforward: int = 2048, \n",
        "        dropout: float = 0.1,\n",
        "        device: str = 'cpu' \n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.device= device\n",
        "\n",
        "    def forward(self, src: Tensor, mask: Tensor) -> Tensor: ##可以传入mask\n",
        "        seq_len, dimension = src.size(1), src.size(2)\n",
        "        pos=position_encoding(seq_len, dimension) #\n",
        "        pos=pos.to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')) # load data to gpu\n",
        "        src += pos\n",
        "        for layer in self.layers:\n",
        "            src = layer(src,mask)  ##传入mask\n",
        "\n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11cf4eb7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:38:43.764147Z",
          "iopub.status.busy": "2022-12-22T11:38:43.763433Z",
          "iopub.status.idle": "2022-12-22T11:38:43.770658Z",
          "shell.execute_reply": "2022-12-22T11:38:43.770091Z",
          "shell.execute_reply.started": "2022-12-22T11:38:43.764119Z"
        },
        "id": "11cf4eb7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# start a trial of gpt model testing\n",
        "class GPT(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        num_encoder_layers: int = 4,\n",
        "        dim_model: int = 512, \n",
        "        num_heads: int = 8, \n",
        "        dim_feedforward: int = 2048//2, \n",
        "        dropout: float = 0.1, \n",
        "        activation: nn.Module = nn.ReLU(),\n",
        "        n_vocab: int=4,\n",
        "        device: str = 'cpu'\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(n_vocab, dim_model)\n",
        "        \n",
        "        self.encoder = TransformerEncoder(\n",
        "            num_layers=num_encoder_layers,\n",
        "            dim_model=dim_model,\n",
        "            num_heads=num_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            device= device\n",
        "        )\n",
        "\n",
        "        self.out = nn.Linear(dim_model, n_vocab)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, src: Tensor, mask: Tensor) -> Tensor: ##传入mask\n",
        "        emb=self.embedding(src)\n",
        "        enc=self.encoder(emb,mask) ##传入mask\n",
        "        out=self.out(enc)\n",
        "        \n",
        "        return out ##no need softmax, nn.cross_entropy take care of it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11785966",
      "metadata": {
        "id": "11785966"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a7b865",
      "metadata": {
        "id": "e1a7b865"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "02903460",
      "metadata": {
        "id": "02903460"
      },
      "source": [
        "# load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "919aebd4",
      "metadata": {
        "id": "919aebd4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9caecf34",
      "metadata": {
        "id": "9caecf34"
      },
      "source": [
        "# train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf2b36ea",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:38:46.638269Z",
          "iopub.status.busy": "2022-12-22T11:38:46.637315Z",
          "iopub.status.idle": "2022-12-22T11:38:46.644301Z",
          "shell.execute_reply": "2022-12-22T11:38:46.643699Z",
          "shell.execute_reply.started": "2022-12-22T11:38:46.638229Z"
        },
        "id": "bf2b36ea",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Generates a square matrix where the each row allows one word more to be seen\n",
        "\n",
        "def generate_masks(src):\n",
        "    seq_len= src.size(1)\n",
        "\n",
        "    pad_int= [int(seq_len-i) for i in src.count_nonzero(dim=1)]\n",
        "\n",
        "    mask = torch.tril(torch.ones(seq_len, seq_len) == 1) # Lower triangular matrix\n",
        "    mask = mask.float()\n",
        "    mask = mask.masked_fill(mask == 0, -1e9) # Convert zeros to -1e9\n",
        "    mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
        "\n",
        "    mask_arr=[]\n",
        "    for i in pad_int:\n",
        "      mask[:,-(i):]= -1e9\n",
        "      mask_arr.append(mask)\n",
        "\n",
        "    masks=torch.cat(tuple(mask_arr),dim=0)\n",
        "    masks=masks.reshape(src.size(0),seq_len,seq_len)\n",
        "    \n",
        "    return masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8d45f2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-22T11:38:47.651961Z",
          "iopub.status.busy": "2022-12-22T11:38:47.651754Z",
          "iopub.status.idle": "2022-12-22T11:38:47.662317Z",
          "shell.execute_reply": "2022-12-22T11:38:47.661729Z",
          "shell.execute_reply.started": "2022-12-22T11:38:47.651937Z"
        },
        "id": "9f8d45f2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "######\n",
        "def loss_masked(output, src, loss_fn):\n",
        "    nonpad_int=src.count_nonzero(dim=1)\n",
        "    # discard pad elements\n",
        "    res=[]\n",
        "    for k,item in enumerate(nonpad_int):\n",
        "        res.append(src[k][:int(item)])\n",
        "\n",
        "    loss_res=0\n",
        "    for i in range(src.size(0)):\n",
        "        loss_res+=loss_fn(output[i], src[i])\n",
        "\n",
        "    return loss_res/src.size(0)\n",
        "######\n",
        "\n",
        "def train(model, data, batch_size):\n",
        "    torch.manual_seed(0)\n",
        "    #model.to(device)\n",
        "    model = nn.DataParallel(model)\n",
        "    torch.cuda.set_device(0)\n",
        "    model.cuda(0)\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss().cuda(0)  ## this is for classification\n",
        "    opt = torch.optim.SGD(model.parameters(), 1e-4)\n",
        "\n",
        "\n",
        "    n=len(data)// batch_size\n",
        "\n",
        "    # prepare for next word prediction\n",
        "    t0=time.time()\n",
        "\n",
        "    for i in range(n):\n",
        "        t0= time.time()\n",
        "        src=data[batch_size*i:batch_size*(i+1)]\n",
        "        src=torch.tensor(src).long()\n",
        "\n",
        "        _seq=src[:,:-1]\n",
        "        seq_=src[:,1:]\n",
        "        masks=generate_masks(_seq)\n",
        "\n",
        "        \n",
        "        #put to gpu\n",
        "        _seq=_seq.cuda(non_blocking=True)\n",
        "        seq_=seq_.cuda(non_blocking=True)\n",
        "        masks=masks.cuda(non_blocking=True)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(_seq,masks)\n",
        "        #print(outputs[0].shape)\n",
        "        #print(seq_[0].shape)\n",
        "        loss=loss_masked(outputs,seq_, loss_fn) # next word prediction\n",
        "\n",
        "        # the part of padding loss should be removed before backprop\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        #print(loss.detach().item())\n",
        "        t1=time.time()\n",
        "        print(t1-t0)\n",
        "        \n",
        "        #if i%10000==0: np.savetxt('./torch_save_model/gpt_loss_%d.csv'%(i), np.array([loss.detach().item()]))\n",
        "\n",
        "    # save model parameters after finish training model\n",
        "    torch.save(model, \"./model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90873858",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "execution": {
          "iopub.execute_input": "2022-12-22T11:39:36.510535Z",
          "iopub.status.busy": "2022-12-22T11:39:36.509828Z",
          "iopub.status.idle": "2022-12-22T11:40:27.564117Z",
          "shell.execute_reply": "2022-12-22T11:40:27.563087Z",
          "shell.execute_reply.started": "2022-12-22T11:39:36.510502Z"
        },
        "id": "90873858",
        "outputId": "8468b491-123f-4631-9d1f-70ab1475bcd2",
        "tags": []
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31mKeyboardInterrupt\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# the entry to start training the model, with data, with specified parameter\n",
        "if __name__ == '__main__':\n",
        "    # load cpu or gpu name\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # model parameter\n",
        "    MODEL_DIM = 256\n",
        "    N_LAYER = 4\n",
        "    N_HEAD = 8\n",
        "    # training argument\n",
        "    batch_size = 2\n",
        "    # load data\n",
        "    ut = utils(path='./file00')\n",
        "    d, v2i=ut.get_idx_sentence()\n",
        "    n_vocab= len(v2i)\n",
        "    m= GPT(num_encoder_layers= N_LAYER,dim_model= MODEL_DIM, num_heads= N_HEAD, n_vocab=n_vocab, device=device)\n",
        "    # start training\n",
        "    train(m, d, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clear memory manually"
      ],
      "metadata": {
        "id": "VnEvKHNm7eaa"
      },
      "id": "VnEvKHNm7eaa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bc86302-ffe4-4d23-80bf-638aa958032d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-11-24T12:44:20.911607Z",
          "iopub.status.busy": "2022-11-24T12:44:20.910946Z",
          "iopub.status.idle": "2022-11-24T12:44:21.287722Z",
          "shell.execute_reply": "2022-11-24T12:44:21.287145Z",
          "shell.execute_reply.started": "2022-11-24T12:44:20.911577Z"
        },
        "tags": [],
        "id": "3bc86302-ffe4-4d23-80bf-638aa958032d",
        "outputId": "9fd40280-4994-46a3-ee18-b2711231bff4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e9711f7-a54f-4b10-a4d5-e11d6f256540",
      "metadata": {
        "id": "2e9711f7-a54f-4b10-a4d5-e11d6f256540"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf2e1a25-a864-4e1b-bded-db6a890e254a",
      "metadata": {
        "id": "cf2e1a25-a864-4e1b-bded-db6a890e254a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}